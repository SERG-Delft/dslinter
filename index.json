[{"content":"Description Context Data structures can be manipulated in mainly two different approaches: 1) by applying the changes to a copy of the data structure and leaving the original object intact, or 2) by changing the existing data structure (also known as in-place).\nProblem Some methods can adopt in-place by default, while others return a copy. If the developer assumes an in-place approach, he will not assign the returned value to any variable. Hence, the operation will be executed, but it will not affect the final outcome. For example, when using the Pandas library, the developer may not assign the result of df.dropna() to a variable. He may assume that this API will make changes on the original DataFrame and not set the in-place parameter to be True either. The original DataFrame will not be updated in this way. In the \u0026quot;TensorFlow Bugs\u0026quot; replication package, we also found an example where the developer thought np.clip() is an in-place operation and used it without assigning it to a new variable.\nSolution We suggest developers check whether the result of the operation is assigned to a variable or the in-place parameter is set in the API. Some developers hold the view that the in-place operation will save memory. However, this is a misconception in the Pandas library because the copy of the data is still created. In PyTorch, the in-place operation does save GPU memory, but it risks overwriting the values needed to compute the gradient.\nType Generic\nExisting Stage Data Cleaning\nEffect Error-prone\nExample ### NumPy import numpy as np zhats = [2, 3, 1, 0] - np.clip(zhats, -1, 1) + zhats = np.clip(zhats, -1, 1)  ### Pandas import pandas as pd df = pd.DataFrame([-1]) - df.abs() + df = df.abs() Source: Paper  MPA Haakman. 2020. Studying the Machine Learning Lifecycle and ImprovingCode Quality of Machine Learning Applications.  Grey Literature  https://towardsdatascience.com/in-place-operations-in-pytorch-f91d493e970e https://github.com/bamos/dcgan-completion.tensorflow/commit/e8b930501dffe01db423b6ca1c65d3ac54f27223  GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/1-in-place-apis-misused/","summary":"Remember to assign the result of an operation to a variable or set the in-place parameter in the API.","title":"In-Place APIs Misused"},{"content":"Description Context Loops are typically time-consuming and verbose, while developers can usually use some vectorized solutions to replace the loops.\nProblem As stated in the Pandas documentation: \u0026ldquo;Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided\u0026rdquo;. In EffectiveTensorflow github repository, it is also stated that the slicing operation with loops in TensorFlow is slow, and there is a substitute for better performance.\nSolution Machine learning applications are typically data-intensive, requiring operations on data sets rather than an individual value. Therefore, it is better to adopt a vectorized solution instead of iterating over data. In this way, the program runs faster and code complexity is reduced, resulting in more efficient and less error-prone code. Pandas\u0026rsquo; built-in methods (e.g., join, groupby) are vectorized. It is therefore recommended to use Pandas built-in methods as an alternative to loops. In TensorFlow, using the tf.reduce_sum() API to perform reduction operation is much faster than combining slicing operation and loops.\nType Generic\nExisting Stage Data Cleaning\nEffect Efficiency\nExample ### Pandas import pandas as pd df = pd.DataFrame([1, 2, 3]) - result = [] - for index, row in df.iterrows(): - result.append(row[0] + 1) - result = pd.DataFrame(result) + result = df.add(1)  ### TensorFlow 2 import tensorflow as tf x = tf.random.uniform([500, 10]) - z = tf.zeros([10]) - for i in range(500): - z += x[i] + z = tf.reduce_sum(x, axis=0) Source: Paper  MPA Haakman. 2020. Studying the Machine Learning Lifecycle and ImprovingCode Quality of Machine Learning Applic  Grey Literature GitHub Commit  https://github.com/tensorflow/models/commit/90f63a1e1653bfa17fde8260a4aa20231b269b7d  Stack Overflow Documentation  https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#iteration  ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/2-unnecessary-iteration/","summary":"Avoid unnecessary iterations. Use vectorized solutions instead of loops.","title":"Unnecessary Iteration"},{"content":"Description Context Feature scaling is a method of aligning features from various value ranges to the same range.\nProblem There are many operations sensitive to feature scaling, including Principal Component Analysis (PCA), Support Vector Machine (SVM), Stochastic Gradient Descent (SGD), Multi-layer Perceptron classifier and L1 and L2 regularization. Missing scaling can lead to a wrong conclusion. For example, if one variable is on a larger scale than another, it will dominate the PCA procedure. Therefore, PCA without feature scaling can produce a wrong principal component result.\nSolution To avoid bugs, whether feature scaling is added before scaling-sensitive operations should be checked.\nType Generic\nExisting Stage Feature Engineering\nEffect Error-prone\nExample ### Scikit-Learn PCA from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.decomposition import PCA from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score + from sklearn.preprocessing import StandardScaler  # Code source: Tyler Lanigan \u0026lt;tylerlanigan@gmail.com\u0026gt; # Sebastian Raschka \u0026lt;mail@sebastianraschka.com\u0026gt; # License: BSD 3 clause  # Make a train/test split using 30% test size RANDOM_STATE = 42 features, target = load_wine(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(  features, target, test_size=0.30, random_state=RANDOM_STATE )  # Fit to data and predict using pipeline - clf = make_pipeline(PCA(n_components=2), GaussianNB()) + clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB()) clf.fit(X_train, y_train) pred_test = clf.predict(X_test) ac = accuracy_score(y_test, pred_test)   ### Scikit-Learn SVC from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.svm import SVC + from sklearn.pipeline import make_pipeline + from sklearn.preprocessing import StandardScaler  # Make a train/test split using 30% test size RANDOM_STATE = 42 features, target = load_wine(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(  features, target, test_size=0.30, random_state=RANDOM_STATE )  # Fit to data and predict using pipelined GNB and PCA - clf = SVC() + clf = make_pipeline(StandardScaler(), SVC()) clf.fit(X_train, y_train) pred_test = clf.predict(X_test) ac = accuracy_score(y_test, pred_test) Source: Paper Grey Literature  https://towardsdatascience.com/my-machine-learning-model-is-perfect-9a7928e0f604 https://ml.posthaven.com/machine-learning-done-wrong  GitHub Commit Stack Overflow  https://stackoverflow.com/questions/17455302/gridsearchcv-extremely-slow-on-small-dataset-in-scikit-learn/23813876#23813876  Documentation  https://scikit-learn.org/stable/modules/preprocessing.html https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-download-auto-examples-preprocessing-plot-scaling-importance-py  ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/3-no-scaling-before-scaling-sensitive-operation/","summary":"Check whether feature scaling is added before scaling-sensitive operations.","title":"No Scaling Before Scaling-sensitive Operation"},{"content":"Description Context Hyperparameters are usually set before the actual learning process begins and control the learning process. These parameters directly influence the behavior of the training algorithm and therefore have a significant impact on the model\u0026rsquo;s performance.\nProblem The default parameters of learning algorithm APIs may not be optimal for a given data or problem, and may lead to local optima. In addition, while the default parameters of a machine learning library may be adequate for some time, these default parameters may change in new versions of the library. Furthermore, not setting the hyperparameters explicitly is inconvenient for replicating the model in a different programming language.\nSolution Hyperparameters should be set explicitly and tuned for improving the result\u0026rsquo;s quality and reproducibility.\nType Generic\nExisting Stage Model Training\nEffect Error-prone \u0026amp; Reproducibility\nExample ### Scikit-Learn from sklearn.cluster import KMeans  - kmeans = KMeans() + kmeans = KMeans(n_clusters=8, random_state=0) + # Or, ideally: + kmeans = KMeans(n_clusters=8, + init=\u0026#39;k-means++\u0026#39;, n_init=10, + max_iter=300, tol=0.0001, + precompute_distances=\u0026#39;auto\u0026#39;, + verbose=0, random_state=0, + copy_x=True, n_jobs=1, + algorithm=\u0026#39;auto\u0026#39;)  ### PyTorch import torch import numpy as np from kmeans_pytorch import kmeans  # data data_size, dims, num_clusters = 1000, 2, 3 x = np.random.randn(data_size, dims) / 6 x = torch.from_numpy(x)  # kmeans - cluster_ids_x, cluster_centers = kmeans(X=x, num_clusters=num_clusters) + cluster_ids_x, cluster_centers = kmeans( + X=x, num_clusters=num_clusters, distance=\u0026#39;euclidean\u0026#39;, device=torch.device(\u0026#39;cpu\u0026#39;) + ) Source: Paper  MPA Haakman. 2020. Studying the Machine Learning Lifecycle and ImprovingCode Quality of Machine Learning Applications. (2020). Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, and D Sculley. 2017. TheML test score: A rubric for ML production readiness and technical debt reduction.In2017 IEEE International Conference on Big Data (Big Data). IEEE, 1123–1132. Gopi Krishnan Rajbahadur, Gustavo Ansaldi Oliva, Ahmed E Hassan, and Juer-gen Dingel. 2019. Pitfalls Analyzer: Quality Control for Model-Driven DataScience Pipelines. In2019 ACM/IEEE 22nd International Conference on ModelDriven Engineering Languages and Systems (MODELS). IEEE, 12–22. Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, AndreaStocco, and Paolo Tonella. 2020. Taxonomy of real faults in deep learning sys-tems. InProceedings of the ACM/IEEE 42nd International Conference on SoftwareEngineering. 1110–1121.  Grey Literature GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/4-hyperparameter-not-explicitly-set/","summary":"Hyperparameters should be set explicitly.","title":"Hyperparameter not Explicitly Set"},{"content":"Description Context Machine learning training is memory-consuming, and the machine\u0026rsquo;s memory is always limited by budget.\nProblem If the machine runs out of memory while training the model, the training will fail.\nSolution Some APIs are provided to alleviate the run-out-of-memory issue in deep learning libraries. TensorFlow\u0026rsquo;s documentation notes that if the model is created in a loop, it is suggested to use clear\\_session() in the loop. Meanwhile, the GitHub repository pytorch-styleguide recommends using .detach() to free the tensor from the graph whenever possible. The .detach() API can prevent unnecessary operations from being recorded and therefore can save memory. Developers should check whether they use this kind of APIs to free the memory whenever possible in their code.\nType Generic\nExisting Stage Model Training\nEffect Memory Issue\nExample ### TensorFlow import tensorflow as tf for _ in range(100): + tf.keras.backend.clear_session()  model = tf.keras.Sequential([tf.keras.layers.Dense(10) for _ in range(10)]) Source: Paper Grey Literature  https://github.com/IgorSusmelj/pytorch-styleguide  GitHub Commit Stack Overflow Documentation  https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session https://stackoverflow.com/questions/42495930/tensorflow-oom-on-gpu  ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/5-memory-not-freed/","summary":"Free memory in time.","title":"Memory not Freed"},{"content":"Description Context Using deterministic algorithms can improve reproducibility.\nProblem The non-deterministic algorithm cannot produce repeatable results, which is inconvenient for debugging.\nSolution Some libraries provide APIs for developers to use the deterministic algorithm. In PyTorch, it is suggested to set torch.use_deterministic_algorithms(True) when debugging. However, the application will perform slower if this option is set, so it is suggested not to use it in the deployment stage.\nType Generic\nExisting Stage Model Training\nEffect Reproducibility\nExample ### PyTorch import torch + torch.use_deterministic_algorithms(True) Source: Paper Grey Literature GitHub Commit Stack Overflow Documentation  https://pytorch.org/docs/stable/notes/randomness.html  ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/6-deterministic-algorithm-option-not-used/","summary":"Set deterministic algorithm option to \u003ccode\u003eTrue\u003c/code\u003e during the development process, and use the option that provides better performance in the production.","title":"Deterministic Algorithm Option Not Used"},{"content":"Description Context In deep learning, the value of the variable changes during training. The variable may turn into an invalid value for another operation in this process.\nProblem Several posts on Stack Overflow talk about the bugs that are not easy to discover caused by the input of the log function approaching zero. In this kind of program, the input variable turns to zero and becomes an invalid value for tf.log(), which raises an error during the training process. However, the error\u0026rsquo;s stack trace did not directly point to the line of code that the bug exists. This problem is not easy to debug and may take a long training time to find.\nSolution The developer should check the input for tf.log() API and add a mask to avoid the invalid value. For example, developer can change tf.log(x) to tf.log(tf.clip_by_value(x,1e-10,1.0)). If the value of x becomes zero, i.e., lower than the lowest bound 1e-10, the tf.clip_by_value() API will act as a mask and outputs 1e-10. The same applies to other invalid value situations. It will save time and effort if the developer could identify this smell before the code run into errors.\nType Generic\nExisting Stage Model Training\nEffect Error-prone\nExample ### TensorFlow 1.x from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf mnist = input_data.read_data_sets(\u0026#34;MNIST_data/\u0026#34;, one_hot=True) sess = tf.InteractiveSession()  def weight_variable(shape):  initial = tf.truncated_normal(shape, stddev=0.1)  return tf.Variable(initial)  def bias_Variable(shape):  initial = tf.constant(0.1, shape=shape)  return tf.Variable(initial)  def conv2d(x, W):  return tf.nn.conv2d(x, W, strides=[1,1,1,1],padding=\u0026#34;SAME\u0026#34;)  def max_pool_2x2(x):  return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1],padding=\u0026#34;SAME\u0026#34;)  x = tf.placeholder(tf.float32, [None, 784]) y_ = tf.placeholder(tf.float32, [None, 10])  x_image = tf.reshape(x, [-1, 28, 28, 1])  W_conv1 = weight_variable([5,5,1,32]) b_conv1 = bias_Variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1) h_pool1 = max_pool_2x2(h_conv1)  W_conv2 = weight_variable([5,5,32,64]) b_conv2 = bias_Variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2) h_pool2 = max_pool_2x2(h_conv2)  W_fc1 = weight_variable([7*7*64,1024]) b_fc1 = bias_Variable([1024]) h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)  keep_prob = tf.placeholder(tf.float32) h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)  W_fc2 = weight_variable([1024,10]) b_fc2 = bias_Variable([10]) y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)  - cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_conv),reduction_indices=[1])) + cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)),reduction_indices=[1])) train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)  correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))  tf.global_variables_initializer().run()  for i in range(20000):  batch = mnist.train.next_batch(50)  if i % 100 == 0:  train_accuracy = accuracy.eval(feed_dict = {x:batch[0] , y_:batch[1], keep_prob: 1.0})  print(\u0026#34;step %d,train accuracy %g\u0026#34;%(i,train_accuracy))  train_step.run(feed_dict = {x:batch[0] , y_:batch[1], keep_prob: 0.5})  print(\u0026#34;test accuracy %g\u0026#34;%accuracy.eval(feed_dict = {x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0})) Source: Paper  Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, AndreaStocco, and Paolo Tonella. 2020. Taxonomy of real faults in deep learning sys-tems. InProceedings of the ACM/IEEE 42nd International Conference on SoftwareEngineering. 1110–1121. Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and Lu Zhang. 2018.An empirical study on TensorFlow program bugs. InProceedings of the 27th ACMSIGSOFT International Symposium on Software Testing and Analysis. 129–140.  Grey Literature GitHub Commit Stack Overflow  https://stackoverflow.com/questions/33712178/tensorflow-nan-bug https://stackoverflow.com/questions/33699174/tensorflows-relugrad-claims-input-is-not-finite https://stackoverflow.com/questions/39487825/tensorflow-convolutionary-net-grayscale-vs-black-white-training https://stackoverflow.com/questions/35078027/implement-mlp-in-tensorflow  Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/7-missing-the-mask-of-invalid-value/","summary":"Add a mask for possible invalid values. For example, developers should add a mask for the input for \u003ccode\u003etf.log()\u003c/code\u003e API.","title":"Missing the Mask of Invalid Value"},{"content":"Description Context There are several scenarios involving random seeds. In some algorithms, randomness is inherently involved in the training process. For the cross-validation process in the model evaluation stage, the dataset split by some library APIs can vary depending on random seeds.\nProblem If the random seed is not set, the result will be irreproducible, which increases the debugging effort. In addition, it will be difficult to replicate the study based on the previous one. For example, in Scikit-Learn, if the random seed is not set, the random forest algorithm may provide a different result every time it runs, and the dataset split by cross-validation splitter will also be different in the next run.\nSolution It is recommended to set global random seed first for reproducible results in Scikit-Learn, Pytorch, Numpy and other libraries where a random seed is involved. Specifically, DataLoader in PyTorch needs to be set with a random seed to ensure the data is split and loaded in the same way every time running the code.\nType Generic\nExisting Stage Model Training \u0026amp; Model Evaluation\nEffect Reproducibility\nExample ### python in general import random + random.seed(0)  ### Tensorflow import tensoflow as tf + tf.random.set_seed(0)  ### PyTorch import torch + torch.manual_seed(0)  ### Scikit-Learn from sklearn.model_selection import KFold + rng = 0 - kf = KFold(random_state=None) + kf = KFold(random_state=rng)  ### NumPy import numpy as np + np.random.seed(0) Source: Paper Grey Literature  https://towardsdatascience.com/my-machine-learning-model-is-perfect-9a7928e0f604 https://github.com/IgorSusmelj/pytorch-styleguide  GitHub Commit Stack Overflow  https://stackoverflow.com/questions/57416925/best-practices-for-generating-a-random-seeds-to-seed-pytorch  Documentation  https://pytorch.org/docs/stable/notes/randomness.html  ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/8-randomness-uncontrolled/","summary":"Set random seed explicitly during the development process whenever a possible random procedure is involved in the application.","title":"Randomness Uncontrolled"},{"content":"Description Context The data leakage occurs when the data used for training a machine learning model contains prediction result information.\nProblem Data leakage frequently leads to overly optimistic experimental outcomes and poor performance in real-world usage.\nSolution There are two main sources of data leakage: leaky predictors and a leaky validation strategy. Leaky predictors are the cases in which some features used in training are modified or generated after the goal value has been achieved. This kind of data leakage can only be inspected at the data level rather than the code level. Leaky validation strategy refers to the scenario where training data is mixed with validation data. This fault can be checked at the code level. One best practice in Scikit-Learn is to use the Pipeline() API to prevent data leakage.\nType Generic\nExisting Stage Model Evaluation\nEffect Error-prone\nExample ### Scikit-Learn from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score import numpy as np + from sklearn.pipeline import make_pipeline  n_samples, n_features, n_classes = 200, 10000, 2 rng = np.random.RandomState(42) X = rng.standard_normal((n_samples, n_features)) y = rng.choice(n_classes, n_samples)  - X_selected = SelectKBest(k=25).fit_transform(X, y) - X_train, X_test, y_train, y_test = train_test_split(X_selected, y, random_state=42) - gbc = GradientBoostingClassifier(random_state=1) - gbc.fit(X_train, y_train) - y_pred = gbc.predict(X_test) + X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) + pipeline = make_pipeline(SelectKBest(k=25), GradientBoostingClassifier(random_state=1)) + pipeline.fit(X_train, y_train) + y_pred = pipeline.predict(X_test)  accuracy_score(y_test, y_pred) Source: Paper  MPA Haakman. 2020. Studying the Machine Learning Lifecycle and Improving Code Quality of Machine Learning Applications. (2020).  Grey Literature GitHub Commit Stack Overflow  https://stackoverflow.com/questions/43816718/keras-regression-using-scikit-learn-standardscaler-with-pipeline-and-without-pip/43816833#43816833  Documentation  https://scikit-learn.org/stable/common_pitfalls.html  ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/9-data-leakage/","summary":"Use \u003ccode\u003ePipeline()\u003c/code\u003e API in Scikit-Learn or check data segregation carefully when using other libraries to prevent data leakage.","title":"Data Leakage"},{"content":"Description Context The performance of the machine learning model can be measured by different metrics, including threshold-dependent metrics (e.g., F-measure) or threshold-independent metrics (e.g., Area Under the Curve (AUC)).\nProblem Choosing a specific threshold is tricky and can lead to a less-interpretable result.\nSolution Threshold-independent metrics are more robust and should be preferred over threshold-dependent metrics.\nType Generic\nExisting Stage Model Evaluation\nEffect Robustness\nExample ### Scikit-Learn from sklearn import metrics y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] metrics.f1_score(y_true, y_pred, average=\u0026#39;weighted\u0026#39;)  + y = [1, 1, 2, 2] + pred = [0.1, 0.4, 0.35, 0.8] + fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2) + print(metrics.auc(fpr, tpr)) Source: Paper  Gopi Krishnan Rajbahadur, Gustavo Ansaldi Oliva, Ahmed E Hassan, and JuergenDingel. 2019. Pitfalls Analyzer: Quality Control for Model-Driven Data SciencePipelines. In2019 ACM/IEEE 22nd International Conference on Model DrivenEngineering Languages and Systems (MODELS). IEEE, 12–22.  Grey Literature GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/10-threshold-dependent-validation/","summary":"Use threshold-independent metrics instead of threshold-dependent ones in model evaluation.","title":"Threshold-Dependent Validation"},{"content":"Description Context NaN equivalence comparison in NumPy and Pandas behaves differently from Python None equivalence comparison.\nProblem While None == None evaluates to True, np.nan == np.nan evaluates to False in NumPy. As Pandas treats None like np.nan for simplicity and performance reasons, a comparison of DataFrame elements with np.nan always returns False. If the developer is not aware of this, it may lead to unintentional bugs in the code.\nSolution Developers need to be careful when using the NaN comparison in Numpy and Pandas.\nType API-Specific\nExisting Stage Data Cleaning\nEffect Error-prone\nExample ### Pandas \u0026amp; NumPy import pandas as pd - import numpy as np  df = pd.DataFrame([1, None, 3]) - df_is_nan = df == np.nan + df_is_nan = df.isna() Source: Paper  MPA Haakman. 2020. Studying the Machine Learning Lifecycle and ImprovingCode Quality of Machine Learning Application. (2020).  Grey Literature GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/11-nan-equivalence-comparison-misused/","summary":"Be careful when using the \u003ccode\u003eNaN\u003c/code\u003e equivalence comparison in NumPy and Pandas.","title":"NaN Equivalence Comparison Misused"},{"content":"Description Context In Pandas, df[\u0026quot;one\u0026quot;][\u0026quot;two\u0026quot;] and df.loc[:,(\u0026quot;one\u0026quot;,\u0026quot;two\u0026quot;)] give the same result. df[\u0026quot;one\u0026quot;][\u0026quot;two\u0026quot;] is called chain indexing.\nProblem Using chain indexing may cause performance issues as well as prone-to-bug code. For example, when using df[\u0026quot;one\u0026quot;][\u0026quot;two\u0026quot;], Pandas see this operation as two events: call df[\u0026quot;one\u0026quot;] first and call [\u0026quot;two\u0026quot;] based on the result the previous operation gets. On the contrary, df.loc[:,(\u0026quot;one\u0026quot;,\u0026quot;two\u0026quot;)] only perform a single call. In this way, the second approach can be significantly faster than the first one. Furthermore, assigning to the product of chain indexing has inherently unpredictable results. Since Pandas makes no guarantees on whether df[\u0026quot;one\u0026quot;] will return a view or a copy, the assignment may fail.\nSolution Developers using Pandas should avoid using chain indexing.\nType API-Specific\nExisting Stage Data Cleaning\nEffect Error-prone \u0026amp; Efficiency\nExample ### Pandas import pandas as pd df = pd.DataFrame([[1,2,3],[4,5,6]]) col = 1 x = 0 - df[col][x] = 42 + df.loc[x, col] = 42 Source: Paper Grey Literature GitHub Commit Stack Overflow  https://stackoverflow.com/questions/22491628/extrapolate-values-in-pandas-dataframe/35959909#35959909 https://stackoverflow.com/questions/53806570/why-does-one-use-of-iloc-give-a-settingwithcopywarning-but-the-other-doesnt/53807453#53807453  Documentation  https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-view-versus-copy  ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/12-chain-indexing/","summary":"Avoid using chain indexing in Pandas.","title":"Chain Indexing"},{"content":"Description Context In Pandas, df.to_numpy() and df.values() both can turn a DataFrame to a NumPy array.\nProblem As noted in a Stack Overflow post, df.values() has an inconsistency problem. With .values() it is unclear whether the returned value would be the actual array, some transformation of it, or one of the Pandas custom arrays. However, the .values() API has not been not deprecated yet. Although the library developers note it as a warning in the documentation, it does not log a warning or error when compiling the code if we use .value().\nSolution When converting DataFrame to NumPy array, it is better to use df.to_numpy() than df.values().\nType API-Specific\nExisting Stage Data Cleaning\nEffect Consistency\nExample ### NumPy \u0026amp; Pandas import numpy as np import pandas as pd  index = [1, 2, 3, 4, 5, 6, 7] a = [np.nan, np.nan, np.nan, 0.1, 0.1, 0.1, 0.1] b = [0.2, np.nan, 0.2, 0.2, 0.2, np.nan, np.nan] c = [np.nan, 0.5, 0.5, np.nan, 0.5, 0.5, np.nan] df = pd.DataFrame({\u0026#39;A\u0026#39;: a, \u0026#39;B\u0026#39;: b, \u0026#39;C\u0026#39;: c}, index=index) df = df.rename_axis(\u0026#39;ID\u0026#39;) - arr = df.values + arr = df.to_numpy() Source: Paper Grey Literature GitHub Commit Stack Overflow  https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array/54508052#54508052  Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/13-dataframe-coversion-api-misused/","summary":"Use \u003ccode\u003edf.to_numpy()\u003c/code\u003e in Pandas instead of \u003ccode\u003edf.values()\u003c/code\u003e for transform a \u003ccode\u003eDataFrame\u003c/code\u003e to a NumPy array.","title":"Dataframe Coversion API Misused"},{"content":"Description Context When the multiply operation is performed on two-dimensional matrixes, np.matmul() and np.dot() give the same result, which is a matrix.\nProblem In mathematics, the result of the dot product is expected to be a scalar rather than a vector. The np.dot() returns a new matrix for two-dimensional matrixes multiplication, which does not match with its mathematics semantics. Developers sometimes use np.dot() in scenarios where it is not supposed to, e.g., two-dimensional multiplication.\nSolution When the multiply operation is performed on two-dimensional matrixes, np.matmul() is preferred over np.dot() for its clear semantic.\nType API-Specific\nExisting Stage Data Cleaning\nEffect Readability\nExample ### NumPy import numpy as np a = [[1, 0], [0, 1]] b = [[4, 1], [2, 2]] - np.dot(a, b) + np.matmul(a, b) Source: Paper Grey Literature GitHub Commit Stack Overflow  https://stackoverflow.com/questions/54160155/does-np-dot-automatically-transpose-vectors/54161169#54161169  Documentation  https://numpy.org/doc/stable/reference/generated/numpy.dot.html#numpy.dot  ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/14-matrix-multiplication-api-misused/","summary":"When the multiply operation is performed on two-dimensional matrixes, use \u003ccode\u003enp.matmul()\u003c/code\u003e instead of \u003ccode\u003enp.dot()\u003c/code\u003e in NumPy for better semantics.","title":"Matrix Multiplication API Misused"},{"content":"Description Context All columns are selected by default when a DataFrame is imported from a file or other sources. The data type for each column is defined based on the default dtype conversion.\nProblem If the columns are not selected explicitly, it is not easy for developers to know what to expect in the downstream data schema. If the datatype is not set explicitly, it may silently continue the next step even though the input is unexpected, which may cause errors later.\nSolution It is recommended to set their columns and DataType explicitly.\nType API-Specific\nExisting Stage Data Cleaning\nEffect Readability\nExample import pandas as pd df = pd.read_csv(\u0026#39;data.csv\u0026#39;) + df = df[[\u0026#39;col1\u0026#39;, \u0026#39;col2\u0026#39;, \u0026#39;col3\u0026#39;]] Source: Paper Grey Literature  https://github.com/joshlk/pandas_style_guide  GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/15-columns-and-datatype-not-explicitly-set/","summary":"Explicitly select columns and set \u003ccode\u003eDataType\u003c/code\u003e in Pandas.","title":"Columns and DataType Not Explicitly Set"},{"content":"Description Context Developers may need a new empty column in DataFrame.\nProblem If they use zeros or empty strings to initialize a new empty column in Pandas, the ability to use methods such as .isnull() or .notnull() is retained.\nSolution Use NaN value (e.g. np.nan) if a new empty column in a DataFrame is needed. Do not use \u0026ldquo;filler values\u0026rdquo; such as zeros or empty strings.\nType API-Specific\nExisting Stage Data Cleaning\nEffect Robustness\nExample import pandas as pd + import numpy as np  df = pd.DataFrame([]) - df[\u0026#39;new_col_int\u0026#39;] = 0 - df[\u0026#39;new_col_str\u0026#39;] = \u0026#39;\u0026#39; + df[\u0026#39;new_col_float\u0026#39;] = np.nan + df[\u0026#39;new_col_int\u0026#39;] = pd.Series(dtype=\u0026#39;int\u0026#39;) + df[\u0026#39;new_col_str\u0026#39;] = pd.Series(dtype=\u0026#39;object\u0026#39;) Source: Paper Grey Literature  https://github.com/joshlk/pandas_style_guide  GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/16-empty-column-misinitialization/","summary":"When a new empty column is needed in a \u003ccode\u003eDataFrame\u003c/code\u003e in Pandas, use the \u003ccode\u003eNaN\u003c/code\u003e value in Numpy instead of using zeros or empty strings.","title":"Empty Column Misinitialization"},{"content":"Description Context df.merge() API merges two DataFrames in Pandas.\nProblem Although using the default parameter can produce the same result, explicitly specify on and how produce better readability. The parameter on states which columns to join on, and the parameter how describes the join method (e.g., outer, inner). Also, the validate parameter will check whether the merge is of a specified type. If the developer assumes the merge keys are unique in both left and right datasets, but that is not the case, and he does not specify this parameter, the result might silently go wrong. The merge operation is usually computationally and memory expensive. It is preferable to do the merging process in one stroke for performance consideration.\nSolution Developers should explicitly specify on, how and validate parameters for df.merge() API for better readability.\nType API-Specific\nExisting Stage Data Cleaning\nEffect Readability \u0026amp; Error-prone\nExample import pandas as pd df1 = pd.DataFrame({\u0026#39;key\u0026#39;: [\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;, \u0026#39;baz\u0026#39;, \u0026#39;foo\u0026#39;],  \u0026#39;value\u0026#39;: [1, 2, 3, 5]}) df2 = pd.DataFrame({\u0026#39;key\u0026#39;: [\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;, \u0026#39;baz\u0026#39;, \u0026#39;foo\u0026#39;],  \u0026#39;value\u0026#39;: [5, 6, 7, 8]}) - df3 = df1.merge(df2) + df3 = df1.merge( + df2, + how=\u0026#39;inner\u0026#39;, + on=\u0026#39;key\u0026#39;, + validate=\u0026#39;m:m\u0026#39; + ) Source: Paper Grey Literature  https://github.com/joshlk/pandas_style_guide  GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/17-merge-api-parameter-not-explicitly-set/","summary":"Explicitly specify \u003ccode\u003eon\u003c/code\u003e, \u003ccode\u003ehow\u003c/code\u003e and \u003ccode\u003evalidate\u003c/code\u003e parameter for \u003ccode\u003edf.merge()\u003c/code\u003e API in Pandas for better readability.","title":"Merge API Parameter Not Explicitly Set"},{"content":"Description Context Use the broadcasting feature in TensorFlow 2 to be more memory efficient.\nProblem Without broadcasting, tiling a tensor first to match another tensor consumes more memory due to the creation and storage of a middle tiling operation result.\nSolution With broadcasting, it is more memory efficient. However, there is a trade-off in debugging since the tiling process is not explicitly stated. Therefore, it is suggested to be as explicit as possible in operation to alleviate the debugging problem. For example, specify the dimension in reduction operations and when using tf.squeeze().\nType API-Specific\nExisting Stage Model Training\nEffect Efficiency\nExample ### TensorFlow example 1 import tensorflow as tf a = tf.constant([[1., 2.], [3., 4.]]) b = tf.constant([[1.], [2.]]) - c = a + tf.tile(b, [1, 2]) + c = a + b  ### TensorFlow example 2 import tensorflow as tf a = tf.random.uniform([5, 3, 5]) b = tf.random.uniform([5, 1, 6]) - tiled_b = tf.tile(b, [1, 3, 1]) - c = tf.concat([a, tiled_b], 2) - d = tf.keras.layers.Dense(10, activation=tf.nn.relu).apply(c) + pa = tf.keras.layers.Dense(10).apply(a) + pb = tf.keras.layers.Dense(10).apply(b) + d = tf.nn.relu(pa + pb) Source: Paper Grey Literature  https://github.com/vahidk/EffectiveTensorflow  GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/18-broadcasting-feature-not-used/","summary":"Use the broadcasting feature in TensorFlow 2 to be more memory efficient.","title":"Broadcasting Feature Not Used"},{"content":"Description Context Developers may need to change the value of the array in the loops in TensorFlow.\nProblem If the developer initializes an array using tf.constant() and tries to assign a new value to it in the loop to keep it growing, the code will run into an error. The developer can fix this error by the low-level tf.while\\_loop() API. However, it is inefficient coding in this way. A lot of intermediate tensors are built in this process.\nSolution Using tf.TensorArray() for growing array in the loop is a better solution for this kind of problem in TensorFlow 2.\nType API-Specific\nExisting Stage Model Training\nEffect Efficiency \u0026amp; Error-prone\nExample ### TensorFlow import tensorflow as tf @tf.function def fibonacci(n):  a = tf.constant(1)  b = tf.constant(1) - c = tf.constant([1, 1]) + c = tf.TensorArray(tf.int32, n) + c = c.write(0, a) + c = c.write(1, b)   for i in range(2, n):  a, b = b, a + b - c = tf.concat([c, [b]], 0) +\tc = c.write(i, b)  - return c +\treturn c.stack()  n = tf.constant(5) d = fibonacci(n) Source: Paper Grey Literature  https://github.com/vahidk/EffectiveTensorflow  GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/19-tensorarray-not-used/","summary":"Use \u003ccode\u003etf.TensorArray()\u003c/code\u003e in TensorFlow 2 if the value of the array will change in the loop.","title":"TensorArray Not Used"},{"content":"Description Context Both self.net() and self.net.forward() can be used to forward the input into the network in PyTorch.\nProblem In PyTorch, self.net() and self.net.forward() are not identical. The self.net() also deals with all the register hooks, which would not be considered when calling the plain .forward().\nSolution It is recommended to use self.net() rather than self.net.forward().\nType API-Specific\nExisting Stage Model Training\nEffect Robustness\nExample ### PyTorch # 1. Load and normalize CIFAR10 import torch import torchvision import torchvision.transforms as transforms  transform = transforms.Compose(  [transforms.ToTensor(),  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  batch_size = 4  trainset = torchvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=True,  download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,  shuffle=True, num_workers=0)  testset = torchvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=False,  download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,  shuffle=False, num_workers=0)  classes = (\u0026#39;plane\u0026#39;, \u0026#39;car\u0026#39;, \u0026#39;bird\u0026#39;, \u0026#39;cat\u0026#39;,  \u0026#39;deer\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;frog\u0026#39;, \u0026#39;horse\u0026#39;, \u0026#39;ship\u0026#39;, \u0026#39;truck\u0026#39;)  # 2. Define a Convolutional Neural Network import torch.nn as nn import torch.nn.functional as F   class Net(nn.Module):  def __init__(self):  super().__init__()  self.conv1 = nn.Conv2d(3, 6, 5)  self.pool = nn.MaxPool2d(2, 2)  self.conv2 = nn.Conv2d(6, 16, 5)  self.fc1 = nn.Linear(16 * 5 * 5, 120)  self.fc2 = nn.Linear(120, 84)  self.fc3 = nn.Linear(84, 10)   def forward(self, x): - x = self.pool.forward(F.relu(self.conv1(x))) + x = self.pool(F.relu(self.conv1(x)))  x = self.pool(F.relu(self.conv2(x)))  x = torch.flatten(x, 1) # flatten all dimensions except batch  x = F.relu(self.fc1(x))  x = F.relu(self.fc2(x))  x = self.fc3(x)  return x   net = Net()  # 3. Define a Loss function and optimizer import torch.optim as optim  criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # 4. Train the network for epoch in range(2): # loop over the dataset multiple times   running_loss = 0.0  for i, data in enumerate(trainloader, 0):  # get the inputs; data is a list of [inputs, labels]  inputs, labels = data   # zero the parameter gradients  optimizer.zero_grad()   # forward + backward + optimize  outputs = net(inputs)  loss = criterion(outputs, labels)  loss.backward()  optimizer.step()   # print statistics  running_loss += loss.item()  if i % 2000 == 1999: # print every 2000 mini-batches  print(f\u0026#39;[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\u0026#39;)  running_loss = 0.0  print(\u0026#39;Finished Training\u0026#39;)  PATH = \u0026#39;./cifar_net.pth\u0026#39; torch.save(net.state_dict(), PATH)  # 5. Test the network on the test data correct = 0 total = 0 # since we\u0026#39;re not training, we don\u0026#39;t need to calculate the gradients for our outputs with torch.no_grad():  for data in testloader:  images, labels = data  # calculate outputs by running images through the network  outputs = net(images)  # the class with the highest energy is what we choose as prediction  _, predicted = torch.max(outputs.data, 1)  total += labels.size(0)  correct += (predicted == labels).sum().item()  print(f\u0026#39;Accuracy of the network on the 10000 test images: {100 * correct // total} %\u0026#39;) Source: Paper Grey Literature  https://github.com/IgorSusmelj/pytorch-styleguide  GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/20-pytorch-call-method-misused/","summary":"Use \u003ccode\u003eself.net()\u003c/code\u003e in PyTorch to forward the input to the network instead of \u003ccode\u003eself.net.forward()\u003c/code\u003e.","title":"Pytorch Call Method Misused"},{"content":"Description Context In PyTorch, calling .eval() means we are going into the evaluation mode and the Dropout layer will be deactivated.\nProblem If the training mode did not toggle back in time, the Dropout layer would not be used in some data training and thus affect the training result.\nSolution Developers should call the training mode in the right place to avoid forgetting to switch back to the training mode after the inference step.\nType API-Specific\nExisting Stage Model Training\nEffect Error-prone\nExample # PyTorch # 1. Load and normalize CIFAR10 import torch import torchvision import torchvision.transforms as transforms  transform = transforms.Compose(  [transforms.ToTensor(),  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  batch_size = 4  trainset = torchvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=True,  download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,  shuffle=True, num_workers=0)  testset = torchvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=False,  download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,  shuffle=False, num_workers=0)  classes = (\u0026#39;plane\u0026#39;, \u0026#39;car\u0026#39;, \u0026#39;bird\u0026#39;, \u0026#39;cat\u0026#39;,  \u0026#39;deer\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;frog\u0026#39;, \u0026#39;horse\u0026#39;, \u0026#39;ship\u0026#39;, \u0026#39;truck\u0026#39;)  # 2. Define a Convolutional Neural Network import torch.nn as nn import torch.nn.functional as F   class Net(nn.Module):  def __init__(self):  super().__init__()  self.conv1 = nn.Conv2d(3, 6, 5)  self.pool = nn.MaxPool2d(2, 2)  self.conv2 = nn.Conv2d(6, 16, 5)  self.fc1 = nn.Linear(16 * 5 * 5, 120)  self.fc2 = nn.Linear(120, 84)  self.fc3 = nn.Linear(84, 10)   def forward(self, x):  x = self.pool(F.relu(self.conv1(x)))  x = self.pool(F.relu(self.conv2(x)))  x = torch.flatten(x, 1) # flatten all dimensions except batch  x = F.relu(self.fc1(x))  x = F.relu(self.fc2(x))  x = self.fc3(x)  return x   net = Net()  # 3. Define a Loss function and optimizer import torch.optim as optim  criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # 4. Train the network for epoch in range(2): # loop over the dataset multiple times   running_loss = 0.0 - net.train()  for i, data in enumerate(trainloader, 0):  # get the inputs; data is a list of [inputs, labels] + net.train()  inputs, labels = data   # zero the parameter gradients  optimizer.zero_grad()   # forward + backward + optimize  outputs = net(inputs)  loss = criterion(outputs, labels)  loss.backward()  optimizer.step()   # print statistics  running_loss += loss.item()  if i % 2000 == 1999: # print every 2000 mini-batches  print(f\u0026#39;[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\u0026#39;)  running_loss = 0.0  # validation  net.eval()  #...  print(\u0026#39;Finished Training\u0026#39;)  PATH = \u0026#39;./cifar_net.pth\u0026#39; torch.save(net.state_dict(), PATH)  # 5. Test the network on the test data correct = 0 total = 0 # since we\u0026#39;re not training, we don\u0026#39;t need to calculate the gradients for our outputs with torch.no_grad():  for data in testloader:  images, labels = data  # calculate outputs by running images through the network  outputs = net(images)  # the class with the highest energy is what we choose as prediction  _, predicted = torch.max(outputs.data, 1)  total += labels.size(0)  correct += (predicted == labels).sum().item()  print(f\u0026#39;Accuracy of the network on the 10000 test images: {100 * correct // total} %\u0026#39;) Source: Paper Grey Literature  https://medium.com/missinglink-deep-learning-platform/most-common-neural-net-pytorch-mistakes-456560ada037  GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/21-training-evaluation-mode-improper-toggling/","summary":"Call the training mode in the appropriate place in PyTorch code to avoid forgetting to toggle back the training mode after the inference step.","title":"Training / Evaluation Mode Improper Toggling"},{"content":"Description Context In PyTorch, optimizer.zero_grad() clears the old gradients from last step, loss_fn.backward() does the back propagation, and optimizer.step() performs weight update using the gradients.\nProblem If optimizer.zero_grad() is not used before loss_fn.backward(), the gradients will be accumulated from all loss_fn.backward() calls and it will lead to the gradient explosion, which fails the training.\nSolution Developers should use optimizer.zero_grad(), loss_fn.backward(), optimizer.step() together in order and should not forget to use optimizer.zero_grad() before loss_fn.backward().\nType API-Specific\nExisting Stage Model Training\nEffect Error-prone\nExample # PyTorch # 1. Load and normalize CIFAR10 import torch import torchvision import torchvision.transforms as transforms  transform = transforms.Compose(  [transforms.ToTensor(),  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  batch_size = 4  trainset = torchvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=True,  download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,  shuffle=True, num_workers=0)  testset = torchvision.datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=False,  download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,  shuffle=False, num_workers=0)  classes = (\u0026#39;plane\u0026#39;, \u0026#39;car\u0026#39;, \u0026#39;bird\u0026#39;, \u0026#39;cat\u0026#39;,  \u0026#39;deer\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;frog\u0026#39;, \u0026#39;horse\u0026#39;, \u0026#39;ship\u0026#39;, \u0026#39;truck\u0026#39;)  # 2. Define a Convolutional Neural Network import torch.nn as nn import torch.nn.functional as F   class Net(nn.Module):  def __init__(self):  super().__init__()  self.conv1 = nn.Conv2d(3, 6, 5)  self.pool = nn.MaxPool2d(2, 2)  self.conv2 = nn.Conv2d(6, 16, 5)  self.fc1 = nn.Linear(16 * 5 * 5, 120)  self.fc2 = nn.Linear(120, 84)  self.fc3 = nn.Linear(84, 10)   def forward(self, x):  x = self.pool(F.relu(self.conv1(x)))  x = self.pool(F.relu(self.conv2(x)))  x = torch.flatten(x, 1) # flatten all dimensions except batch  x = F.relu(self.fc1(x))  x = F.relu(self.fc2(x))  x = self.fc3(x)  return x   net = Net()  # 3. Define a Loss function and optimizer import torch.optim as optim  criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # 4. Train the network for epoch in range(2): # loop over the dataset multiple times   running_loss = 0.0  for i, data in enumerate(trainloader, 0):  # get the inputs; data is a list of [inputs, labels]  inputs, labels = data  + # zero the parameter gradients + optimizer.zero_grad()   # forward + backward + optimize  outputs = net(inputs)  loss = criterion(outputs, labels)  loss.backward()  optimizer.step()   # print statistics  running_loss += loss.item()  if i % 2000 == 1999: # print every 2000 mini-batches  print(f\u0026#39;[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\u0026#39;)  running_loss = 0.0  print(\u0026#39;Finished Training\u0026#39;)  PATH = \u0026#39;./cifar_net.pth\u0026#39; torch.save(net.state_dict(), PATH)  # 5. Test the network on the test data correct = 0 total = 0 # since we\u0026#39;re not training, we don\u0026#39;t need to calculate the gradients for our outputs with torch.no_grad():  for data in testloader:  images, labels = data  # calculate outputs by running images through the network  outputs = net(images)  # the class with the highest energy is what we choose as prediction  _, predicted = torch.max(outputs.data, 1)  total += labels.size(0)  correct += (predicted == labels).sum().item()  print(f\u0026#39;Accuracy of the network on the 10000 test images: {100 * correct // total} %\u0026#39;) Source: Paper Grey Literature  https://medium.com/missinglink-deep-learning-platform/most-common-neural-net-pytorch-mistakes-456560ada037  GitHub Commit Stack Overflow Documentation ","permalink":"https://hynn01.github.io/dslinter/posts/codesmells/22-gradients-not-cleared-before-backward-propagation/","summary":"Use \u003ccode\u003eoptimizer.zero_grad()\u003c/code\u003e, \u003ccode\u003eloss_fn.backward()\u003c/code\u003e, \u003ccode\u003eoptimizer.step()\u003c/code\u003e together in order in PyTorch. Do not forget to use \u003ccode\u003eoptimizer.zero_grad()\u003c/code\u003e before \u003ccode\u003eloss_fn.backward()\u003c/code\u003e to clear gradients.","title":"Gradients Not Cleared before Backward Propagation"}]