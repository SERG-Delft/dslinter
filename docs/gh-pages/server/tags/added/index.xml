<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>added on DSLinter - Linter for Machine Learning - Specific Code Smells</title>
    <link>https://hynn01.github.io/dslinter/tags/added/</link>
    <description>Recent content in added on DSLinter - Linter for Machine Learning - Specific Code Smells</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://hynn01.github.io/dslinter/tags/added/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Initialize Weight to a Constant</title>
      <link>https://hynn01.github.io/dslinter/deprecated/initialize-weight-to-zero/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/initialize-weight-to-zero/</guid>
      <description>Description In neural network, if all the weights are initialized to a constant, i.e., all the neurons starts with the same weight, all the neurons will follow the same gradient during the backward propagation update. As a result, neurons will learn same features in each iterations. In this way, the nueral netwok will provide a poor result.
Type Generic
Existing Stage Model Training
Effect Error-prone
Example # https://github.com/aladdinpersson/Machine-Learning-Collection/blob/a2ee9271b5280be6994660c7982d0f44c67c3b63/ML/Projects/Exploring_MNIST/networks/lenet.py import torch import torch.</description>
    </item>
    
    <item>
      <title>Missing Regularization</title>
      <link>https://hynn01.github.io/dslinter/deprecated/missing-regularization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/missing-regularization/</guid>
      <description>Description In deep learning, regularization can help control overfitting, speed up the training process by dealing with noise and outliers. Developers should check whether they are able to make use of regularization to improve performance.
Type Generic
Existing Stage Model Training
Effect Efficiency
Example ### TensorFlow # Violated Code layer = tf.keras.layers.Dense( 5, input_dim=5, kernel_initializer=&amp;#39;ones&amp;#39;, kernel_regularizer=tf.keras.regularizers.L1(0.01), activity_regularizer=tf.keras.regularizers.L2(0.01)) # Recommended Fix layer = tf.keras.layers.Dense( 5, input_dim=5, kernel_initializer=&amp;#39;ones&amp;#39;) ### PyTorch # Violated Code optimizer = torch.</description>
    </item>
    
    <item>
      <title>Threshold Dependent Validation</title>
      <link>https://hynn01.github.io/dslinter/code-smells/threshold-dependent-validation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/threshold-dependent-validation/</guid>
      <description>Description The performance of the machine learning model can be measured by different metrics, including threshold-dependent metrics(e.g., F-measure) or threshold-independent metrics(e.g., Area Under the receiver operating characteristic curve (AUC)). Choosing a specific threshold is tricky and can lead to a less-interpretable result. Therefore, threshold-independent is more robust and should be preferred over threshold-independent metrics.
Type Generic
Existing Stage Model Evaluation
Effect Robustness
Example ### Scikit-Learn from sklearn import metrics y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] metrics.</description>
    </item>
    
  </channel>
</rss>
